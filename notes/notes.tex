\documentclass[a4paper,12pt,fleqn]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
% Expectation symbol  http://www.guyrutenberg.com/2011/11/19/expectation-symbol-in-latex/
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\I}{\mathbb{I}}
\DeclareMathOperator*{\argmax}{arg\,max}
% usage:
% \myitemize {
%    \item item one
%    \item item two
%    % ...
% }
\newcommand{\myitemize}[1]{\begin{itemize}#1\end{itemize}}

%\usepackage[a4paper]{geometry}
%\newgeometry{left=2cm,bottom=1.5cm,top=1.5cm,right=2cm}
\usepackage[margin=0.2in]{geometry}

\newcommand{\ult}{\vspace{12px}\noindent}
\newcommand{\ul}{\\ \indent -  }
\newcommand{\ull}{\\ \indent\indent - }
\newcommand{\ulll}{\\ \indent\indent\indent - }
\newcommand{\bl}{\\ \indent }
\newcommand{\bll}{\\ \indent\indent }

\newcommand{\partialderivative}[2]{\frac{\partial #1}{\partial #2}}

% ref http://tex.stackexchange.com/questions/9363/how-does-one-insert-a-backslash-or-a-tilde-into-latex
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
\newcommand{\distributedas}{\hspace{0.2cm}\mytilde}

\pagestyle{empty}

\begin{document}

\subsection*{scratch}

\ult {backpropagation}

   \[ y^*(t) = f[x(t)] + \epsilon \]

log likelihood: \[ \log L = \sum_t -\frac{1}{2} || y^*(t) - y(t) || ^2  \]
For output unit: \[ \frac{\partial \log L(n)}{\partial w_{ij}(n)} 
= \sum_t \frac{\partial \log L(t)}{\partial y_i(t)} \frac{\partial y_i(t)}{\partial s_i(t)} \frac{\partial s_i(t)}{\partial w_{ij}(n)} \]
For hidden unit: \[ \frac{\partial \log L(n)}{\partial w_{ij}(n)}
= \sum_t \frac{\partial \log L(t)}{\partial h_i(t)} \frac{\partial h_i(t)}{\partial s_i(t)} \frac{\partial s_i(t)}{\partial w_{ij}(n)} \]
For output units: \[\frac{\partial L(t)}{\partial y_i(t)} = y^*_i(t) - y_i(t)\]
For linear output units: \[ y_i(t) = s_i(t) = \sum_j w_{ij}(n)h_j(t) \]
\[ \frac{\partial y_i(t}{\partial s_i(t) } = 1 \]
\[ \frac{\partial s_i(t)}{\partial w_{ij}(n)} = h_j(t) \]
\[ w_{ij}(n+1) = w_{ij}(n) - \alpha \frac{\partial \log L(n)}{\partial w_{ij}(n)} \]
For hidden unit: \[ \frac{\partial \log L(t)}{\partial w_{ij}(t)} = \sum_k \frac{\partial \log L(t)}{\partial s_k(t)} w_{ki}(n) \]

\ult{sigmoid}
  \ul based on http://www.ics.uci.edu/\~{ }pjsadows/backpropderivation.pdf

cross entropy for single sample is \[ E = - \sum_k ( y^*_k \log( o_k ) + ( 1 - y^*_k) \log(1 - o_k) ) \]
Activation function is logistic: \[ o_i = \frac{1}{1 + \exp(-s_i)} \]
where $s_i$ is sum at a node: \[ s_i = \sum_j( w_{ji}o_j ) \]
we want: \[ \frac{\partial E}{\partial w_{ji}} \]
\[ = \frac{\partial E}{\partial o_i}
\frac{\partial o_i}{\partial s_i}
\frac{\partial s_i}{\partial w_{ji}} \]
\[ \frac{\partial E}{o_i} = - \frac{y^*_i}{o_i} + \frac{1 - y^*_i}{1 - o_i} \]
\[ = - \frac{y^*_i - y^*_i o_i - o_i + y^*_i o_i }       {o_i(1 - o_i)} \] 
\[ = \frac{o_i - y^*_i }       {o_i(1 - o_i )} \] 
\[\frac{\partial o_i}{\partial s_i} = o_i(1-o_i) \]
\[ \frac{\partial E }{\partial s_i} = o_i - y^*_i \]

\[ \frac{\partial E}{\partial s^{l-1}_i} = \sum_k \frac{\partial E}{\partial s^l_k}  \frac{\partial s^l_k}{\partial o^{l-1}_i}   \frac{\partial o^{l-1}_i}{\partial s^{l-1}_i} \]

\[ \frac{\partial E}{\partial s^{l-2}_i} = \sum_k \sum_j \frac{\partial E}{\partial s^l_k}  \frac{\partial s^l_k}{\partial o^{l-1}_j}   \frac{\partial o^{l-1}_j}{\partial s^{l-1}_j}
\frac{\partial s^{l-1}_j}{\partial o^{l-2}_i}   \frac{\partial o^{l-2}_i}{\partial s^{l-2}_i} \]
\[ = \sum_k \sum_j \frac{\partial E}{\partial s^l_k}  w^l_{kj}   \frac{\partial o^{l-1}_j}{\partial s^{l-1}_j}
w^{l-1}_{ji}   \frac{\partial o^{l-2}_i}{\partial s^{l-2}_i} \]

\ult{classification}

class has multinomial distribution: \[ p(y^*(t)|x(t)) = \prod_{k=1}^K y_k(t)^{y_k^*(t)} \]
log likelihood: \[ \log L = \sum_t \sum_k y^*_k (t) \log y_k (t) \]
output units use softmax activation: \[ y_i(t) = \frac{\exp[s_i(t)]}{\sum_k \exp[s_k(t)]} \]
\[ \frac{\partial \log L(n)}{\partial w_{ij}(n)} = \sum_t \frac{\partial \log L(t)}{\partial s_i(t)} \frac{\partial s_i(t)}{\partial w_{ij}(n)} \]
\[ \frac{\partial \log L(t)}{\partial s_i(t)} = \sum_k \frac{\partial \log L(t)}{\partial y_k(t)} \frac{\partial y_k(t)}{\partial s_i(t)} \]
\[ = \sum_k \frac{y^*_k(t)}{y_k(t)} y_k(t) ( \delta_{ik} - y_i(t)) \]
\[ \frac{\partial s_i(t)}{\partial w_{ij}(n)} = h_j(t) \]
Softmax: \[ y_i = \frac{\exp s_{n,i}}{\sum_k \exp s_{n,k}} \]
   \[ \frac{\exp( s_{n,i}) / \exp( \max_j s_{n,j} )}{\sum_k \exp (s_{n,k} )/ \exp( \max_j s_{n,j} ) } \]
   \[ = \frac{\exp( s_{n,i} - \max_j( s_{n,j}))}     {\sum_k \exp( s_{n,k} - \max_j(s_{n,j})   )} \]
Cross entropy for softmax (?): \[ = - \sum_n \sum_k y^*_{n,k} \log y_{n,k} \] where $y^*_{n,k}$ should be 0, except for $y^*_{n,l_n} = 1$, where $l_n$ is the label of instance $n$

\ult{sigmoid, mse (I'm trying myself, might not be correct...)}

\[ L = \sum_k \frac{1}{2}( y^*_k - o_k )^2 \]
\[ o_i = \frac{1}{1 + \exp( - s_i)} \]
\[ s_i = \sum_k w_{ki} o_k \]
\[ \frac{\partial L}{\partial w_{ji} } = \frac{\partial L}{\partial o_i} \frac{\partial o_i}{\partial s_i} 
\frac{\partial s_i}{\partial w_{ji}} \]

\[ \frac{\partial L}{\partial o_i} = 2 \frac{1}{2}(y^*_i - o_i )(-1) \]
\[ = (o_i - y^*_i ) \]
\[ \frac{\partial o_i}{\partial s_i} = o_i(1-o_i) \]
\[ \frac{\partial s_i}{\partial w_{ji}} = o_j \]
\[ \frac{\partial L}{\partial w_{ji}} = (o_k - y^*_k )o_i(1-o_i)o_j \]

\ult{We want to reduce L slightly.}
   \ul therefore, we should modify $w_{ji}$ slightly:
\[ w_{ji}(n+1) = w_{ji}(n) - \alpha \frac{\partial L}{\partial w_{ji}} \]
\[ = w_{ji}(n) - \alpha (o_i - y^*_i )o_i(1-o_i)o_j \]
\[ = w_{ji}(n) - \alpha \text{error}(o_i, y^*_i) \text{derivative}(o_i) \text{output}^{l-1}(o^{l-1}_j) \]

\[ \frac{\partial L}{\partial w^{l-1}_{kj}} = \sum_i \frac{\partial L}{\partial o^l_i}
\frac{\partial o^l_i}{\partial s^l_i} \frac{\partial s^l_i}{o^{l-1}_j}\frac{\partial o^{l-1}_j}{\partial s^{l-1}_j}
\frac{\partial s^{l-1}_j}{\partial w^{l-1}_{kj}} \]
\[ s_i =  \sum_j o^{l-1}_j w^l_{ji} \]
\[ \frac{\partial s^l_i}{o^{l-1}_j} = w^l_{ji} \]
So, \[ \frac{\partial L}{\partial w^{l-1}_{kj}} =\sum_i (o^l_i - y^*_i)o^l_i(1-o^l_i)w^l_{ji}o^{l-1}_{j}(1 - o^{l-1}_{j})o^{l-2}_k \]
\[ = o^{l-1}_{j}(1 - o^{l-1}_{j})o^{l-2}_k \sum_i (o^l_i - y^*_i)o^l_i(1-o^l_i)w^l_{ji} \]
\[ = (\sum_i \text{error}^l \text{deriv}(o^l_i) w^l_{ji}) \text{deriv}(o^{l-1}_j) \text{output}(o^{l-2}_k)   \]
\[ = \text{apparent error}^{l-1}_j(o^l_i, w^l_{ji}, y^*_i) \text{deriv}^{l-1}(o^{l-1}_j) \text{output}^{l-2}(o^{l-2}_k) \]
where \[ \text{apparent error}^{l-1}_j(o^l_i, w^l_{ji}, y^*_i) = \sum_i (o^l_i - y^*_i)o^l_i(1-o^l_i)w^l_{ji} \]
\[ = \sum_i (\text{error}^l_i(o^l_i, y^*_i)\text{deriv}^l_i(o^l_i)w^l_{ji}) \]
\[ = \sum_i ( \frac{\partial L}{\partial o^l_i} \frac{\partial o^l_i}{\partial s^l_i} w^l_{ji} ) \]
\[ = \sum_i ( \frac{\partial L}{\partial o^l_i} \frac{\partial o^l_i}{\partial s^l_i} \frac{\partial s^l_i}{\partial o^{l-1}_{ji}} ) \]
\[ =\sum_i \frac{\partial L}{\partial o^{l-1}_{ji}} \]
\[ =\sum_i \text{apparent error}^l_i \frac{\partial o^l_i}{\partial o^{l-1}_{ji}} \]
\[ =\sum_i \text{apparent error}^l_i \frac{\partial o^l_i}{\partial s^l_i} \frac{\partial s^l_i}{\partial o^{l-1}_{ji}} \]
\[ =\sum_i \text{apparent error}^l_i \frac{\partial o^l_i}{\partial s^l_i} w^l_{ji} \]
eg \[ \frac{\partial o^l_i}{\partial s^l_i} \frac{\partial s^l_i}{\partial o^{l-1}_{ji}} 
= o^l_i(1-o^l_i) w^l_{ji} \]
or, for $\tanh$ \[ \frac{\partial o^l_i}{\partial s^l_i} \frac{\partial s^l_i}{\partial o^{l-1}_{ji}} 
= ( 1 - (o^l_i)^2) w^l_{ji} \]

\ult{Error functions}

Cross entropy: \[ L = - \sum_k ( y^*_k \log( o_k ) + ( 1 - y^*_k) \log(1 - o_k) ) \]
\[ \frac{\partial L}{\partial o_i} = - \frac{y^*_i}{o_i} - \frac{(1 - y^*_i)(-1)}{1 - o_i} \]
\[ = - \frac{y^*_i}{o_i} + \frac{1 - y^*_i}{1 - o_i} \]
\[ = \frac{-y^*_i + y^*_i o_i + o_i - y^*_i o_i}{o_i(1-o_i} \]
\[ = \frac{o_i - y^*_i}{o_i(1-o_i)} \]

Cross-entropy, multiclass: \[ L = - \sum_i y^*_i \log(o_i) \]
\[ \frac{\partial L}{\partial o_i} = - \frac{ y^*_i}{o_i}  \]
when $j = i$: \[ \frac{\partial o_i}{\partial s_j} = o_i(1-o_i) \]
when $j != i$: \[ \frac{\partial o_i}{\partial s_j} = -o_i o_j \]
Generalizing:
\[ \frac{\partial o_i}{\partial s_j} = \delta_{i,j}o_i - o_i o_j \]
\[ = o_i( \delta_{i,j} - o_j ) \]
\[ \frac{\partial E}{\partial s_i} = o_i - y^*_i \]

Squared error: \[ L = \sum_k \frac{1}{2} ( y^*_k - o_k )^2 \]
\[ \frac{\partial L}{\partial o_i } = o_i - y^*_i \]

\ult{Activation functions}

Sigmoid: \[ o_i = \frac{1}{1 + \exp(-s_i)} \]
\[ \frac{\partial o_i}{\partial s_i} = o_i(1 - o_i) \]

Linear: \[ o_i = s_i \]
\[ \frac{\partial o_i}{\partial s_i} = 1 \]

softmax: \[ o_i = \frac{\exp(s_i)}{\sum_k \exp(s_k)} \]
\[ \frac{\partial o_i}{\partial s_i} =  \]

\ult{errors backprop}

My current method:
\[ \text{error}^{l-1} = \sum_j \text{error}^{l}_j \frac{\partial o^{l}_j}{\partial o^{l-1}_i} \]
\[ = \sum_j \frac{\partial L}{\partial o^{l}_j} \frac{\partial o^l_j}{\partial s^l_j} \frac{\partial s^l_j}{\partial o^{l-1}_i} \]
eg \[ = \sum_i \frac{\partial L}{\partial o^{l}_i} ( 1 - (o^{l}_i)^2) ) w^{l}_{ji} \]
\[ \frac{\partial L}{\partial w^l_{ji}} = \frac{\partial L}{\partial o^l_i } \frac{\partial o^l_i}{\partial s^l_i} \frac{\partial s^l_i}{\partial w^l_{ji}} \]
eg \[ = \frac{\partial L}{\partial o^l_i } (1 - (o^l_i)^2) o^{l-1}_j \]

Peter Sadowski "notes on backpropagation" method:
\[ \frac{\partial L}{\partial s^{l-1}_i} = \sum_j \frac{\partial L}{\partial s^l_j} \frac{\partial s^l_j}{\partial o^{l-1}_i} \frac{\partial o^{l-1}_i}{\partial s^{l-1}_i} \]
eg \[ = \sum_j \frac{\partial L}{\partial s^l_j} w^l_{ij} ( 1 - (o^{l-1}_i)^2) \]
\[ \partialderivative{L}{w^l_{ji}} = \partialderivative{L}{s^l_i} \partialderivative{s^l_i}{w^l_{ji}} \]
\[ = \partialderivative{L}{s^l_i} o^{l-1}_j \]

\ult{numerical validation}

\[ w_{ji}(n+1) = w_{ji}(n) + \alpha  \partialderivative{L}{w_{ji}} \]
So \[ \partialderivative{L}{w_{ji}} = \frac{w_{ji}(n+1) - w_{ji}(n)}{\alpha} \]
 \[ L(n+1) \approx L(n) + \Delta w_{ji} \partialderivative{L}{w_{ji}} \]
\[ \Delta(w_{ji}) = \alpha \partialderivative{L}{w_{ji}} \]
Therefore, \[ \partialderivative{L}{w_{ji}} = \frac{\Delta w_{ji}}{\alpha} \]
And so, \[ \Delta L \approx  \frac{(\Delta w_{ji})^2} {\alpha}   \]

\ult{errors backprop compared to possible layer configurations}

using $\partialderivative{E}{s^l_i}$:

\ult{example configuration:}
   \ul expected outputs, $y^*_i$, $L = \sum_i \frac{1}{2} ( y^*_i - o^{l=4}_i )^2 $
   \ul softmax, $o^{l=4}_i$, $o^{l=4}_i=a(s^{l=4}_1,s^{l=4}_2,..,s^{l=4}_k)$, $s^{l=4}_i = o^{l=3}_i$
   \ul fully connected, linear, $o^{l=3}_i$, $s^{l=3}_i$, $w^{l=3}_{ji}$, $o^{l=3}_i=s^{l=3}_i$, $s^{l=3}_i = \sum_k o^{l=2}_k w^{l=3}_{ki}$
   \ul convolutional, relu, $o^{l=2}_i$, $s^{l=2}_i$, $w^{l=2}_{ji}$, $s^{l=2}_i = \sum_k o^{l=1}_k w^{l=2}_{ki}$, $o^{l=2}_i = a(s^{l=2}_i)$
   \ul convolutional, relu, $o^{l=1}_i$, $s^{l=1}_i$, $w^{l=1}_{ji}$, $s^{l=1}_i = \sum_k o^{l=0}_k w^{l=1}_{ki}$, $o^{l=1}_i = a(s^{l=1}_i)$
   \ul input $o^{l=0}_i$

softmax: \[ \partialderivative{L}{o^{l=4}_i} = o^{l=4}_i - y^*_i \]
\[ \partialderivative{o^{l=4}_i}{s^{l=4}_j} = o_i(\delta_{i,j}-o_j) \]
\[ \partialderivative{s^{l=4}_i}{o^{l=3}_i} = 1 \]
\[ \partialderivative{o^{l=3}_i}{s^{l=3}_i} = 1 \]
\[ \partialderivative{s^{l=3}_i}{w^{l=3}_{ji}} = o^{l=2}_j \]
\[ \partialderivative{s^{l=3}_i}{o^{l=2}_j} = w^{l=3}_{ji} \]
\[ \partialderivative{o^{l=2}_i}{s^{l=2}_i} =  \]

\ult{example configuration:}
   \ul expected outputs, $y^*_i$ $\rightarrow$ squared error $E = \sum_k \frac{1}{2}(o_k - y^*_k)^2 $
   \ul fully connected layer, $o^2_i = a^2(s^2_i)$, $a^2(x) = \tanh(x)$ $\partialderivative{\tanh(x)}{x} = 1-(\tanh(x))^2 $, $s^2_i = \sum_k o^1_k w^2_{ki}$
   \ul fully connected layer, $o^1_i = a^1(s^1_i)$, $a^1(x) = \tanh(x)$, $s^1_i = \sum_k o^0_k w^1_{ki}$
   \ul input layer, $o^0_i = i_i$

\[ \partialderivative{E}{o^2_i} = o_i - y^*_i \] \[ \partialderivative{E}{s^2_i} = (o_i - y^*_i ) (1 - (o^2_i)^2) \]

\ult{example configuration:}
   \ul expected outputs, $y^*_i$ $\rightarrow$ cross-entropy error $E = - \sum_k (y^*_k \log o^2_k + (1-y^*_k) \log(1-o^2_k)) $
   \ul fully connected layer, $o^2_i = a^2(s^2_i)$, $a^2(x) = \tanh(x)$ $\partialderivative{\tanh(x)}{x} = 1-(\tanh(x))^2 $, $s^2_i = \sum_k o^1_k w^2_{ki}$
   \ul fully connected layer, $o^1_i = a^1(s^1_i)$, $a^1(x) = \tanh(x)$, $s^1_i = \sum_k o^0_k w^1_{ki}$
   \ul input layer, $o^0_i = i_i$

\[ \partialderivative{E}{o^2_i} = \frac{o^2_i - y^*_i}{o^2_i(1-o^2_i)} \]
\[ \partialderivative{E}{s^2_i} = \frac{o^2_i - y^*_i}{o^2_i(1-o^2_i)} ( 1 - (o^2_i)^2 ) \]

\ult{example configuration:}
   \ul expected outputs, $y^*_i$ $\rightarrow$ cross-entropy error $E = - \sum_k (y^*_k \log o^2_k + (1-y^*_k) \log(1-o^2_k)) $
   \ul fully connected layer, $o^2_i = a^2(s^2_i)$, $a^2(x) = \sigma    (x)$ $\partialderivative{\sigma(x)}{x} = \sigma(x)( 1 - \sigma(x) ) $, $s^2_i = \sum_k o^1_k w^2_{ki}$
   \ul fully connected layer, $o^1_i = a^1(s^1_i)$, $a^1(x) = \tanh(x)$, $s^1_i = \sum_k o^0_k w^1_{ki}$
   \ul input layer, $o^0_i = i_i$

\[ \partialderivative{E}{o^2_i} = \frac{o^2_i - y^*_i}{o^2_i(1-o^2_i)} \]
\[ \partialderivative{E}{s^2_i} = \frac{o^2_i - y^*_i}{o^2_i(1-o^2_i)} o^2_i(1-o^2_i) = o^2_i - y^*_i \]

\ult{example configuration:}
   \ul expected outputs, $y^*_i$, $\rightarrow$ cross-entropy error $E = - \sum_k y^*_k \log o^2_k $
   \ul softmax, $o^{4}_i$, $o^{4}_i=a(s^{4}_1,s^{4}_2,..,s^{4}_k)$, $s^{l=4}_i = o^{3}_i$
   \ul fully connected, linear, $o^{l}_i$, $s^{l}_i$, $w^{l}_{ji}$, $o^{l}_i=s^{l}_i$, $s^{l=3}_i = \sum_k o^{2}_k w^{3}_{ki}$
   \ul convolutional, relu, $o^{l}_i$, $s^{l}_i$, $w^{l}_{ji}$, $s^{l}_i = \sum_k o^{1}_k w^{2}_{ki}$, $o^{2}_i = a(s^{2}_i)$
   \ul convolutional, relu, $o^{1}_i$, $s^{1}_i$, $w^{1}_{ji}$, $s^{1}_i = \sum_k o^{0}_k w^{1}_{ki}$, $o^{1}_i = a(s^{1}_i)$
   \ul input $o^{0}_i = i_i$

\end{document}

